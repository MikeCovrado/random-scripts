#!/usr/bin/python3

# Copywrite 2024 OpenHW Group
# SPDX-License-Identifier: Apache-2.0
# Some functions in this script were wholly or partially generated by Google Gemini.

################################################################################
# url_checkr: search a directory tree, looking for files with a specific file
#             extension.  For each of these files, parse for URLs and ping each
#             to check return code.  List all URLs without a valid return code.
#
# Note that this script can also search for arbitrary string in a file.  In
# such cases the parsing/pining of URLs is not performed.
#
# TODO: proper handling of asciidoc (*.adoc) files.
################################################################################

import os
import sys
import argparse
import re
import urllib.parse
import requests
import chardet

if (sys.version_info < (3,0,0)):
  print ('Requires python 3')
  exit(1)

################################################################################
# Command-line argument processing.  All human generated code.

prefix       = ''            # TODO: consider making this a cla.  e.g.: "Matching File: "
default_dir  = '.'
default_ext  = '.md'
default_pat  = 'http'
exclude_file = 'XXXLICENSE.md' # FIXME: cla for execlude_file does not work...

parser = argparse.ArgumentParser(
        prog = 'URL Checker',
        description = 'Traverse a directory tree for files with a specific extenson and scan for invalid URLs.',
        epilog = 'Changing the default search pattern with --pat disables URL scans.')
parser.add_argument("--debug", help="Display debug messages",                                          action="store_true")
parser.add_argument("--dir",   help="Directory to search from (default: {})".format(default_dir),      default=default_dir)
parser.add_argument("--ext",   help="File extensions to search for (default: {})".format(default_ext), default=default_ext)
parser.add_argument("--exc",   help="File to exclude from search (default: {})".format(exclude_file),  default=exclude_file)
parser.add_argument("--pat",   help="Search pattern (default: {})".format(default_pat),                default=default_pat)

args = parser.parse_args()

root_dir         = args.dir
target_extension = args.ext
search_pattern   = args.pat

################################################################################
# Functions

# Query: Is there a python check of a file encoding to catch non-utf-8 encoded files?
# Notes: multiple manual modifications related to ease integration into the script.
def windows_encoded(filename):
  """
  Determines if a file is Windows-1252 encoded.

  Args:
    filename: The path to the file.

  Returns:
    True if the file is Windows-1252 encoded, False otherwise.
  """
  try:
    with open(filename, 'rb') as f:
      rawdata = f.read()
      result = chardet.detect(rawdata)
      encoding = result['encoding']
      confidence = result['confidence']
      if (args.debug):
        print(f"File encoding: {encoding}, Confidence: {confidence}")
      #return encoding.lower() == 'utf-8'
      return encoding.lower() == 'windows-1252'
  except Exception as e:
    print(f"Error checking file encoding: {e}")
    return False

# Query: Generate a python3 regex that removes one or more occurances of "<br>"
#        from the end of a string.
# Notes: Gemini did not get this right the first time (see below).
def remove_trailing_br_tags(text):
  """Removes one or more occurrences of <br> tags from the end of a string.

  Args:
    text: The input string.

  Returns:
    The string with trailing <br> tags removed.
  """

  # First attempt...
  #return re.sub(r'<br>$', '', text, flags=re.MULTILINE)

  # Second attempt...
  #return re.sub(r'<br>', '', text)

  # Third attempt...
  # This regex will match the string "<br>" at the end of a word. The "\b"
  # character matches a word boundary, and the "\Z" character matches the end of the string.
  #return re.sub(r'\b<br>\Z', '', text)

  # Fourth attempt...
  # (<br>)+: This matches one or more occurrences of "<br>". This allows it to remove multiple "<br>" tags from the end.
  # $: This anchors the match to the end of the string, ensuring that only trailing "<br>" tags are removed.
  return re.sub(r'(<br>)+$', '', text)

# Function to extract actual URL from the form
#          [https://display_url](https://actual_url)
#
# Regular Expressions generated by Gemini
def extract_actual_url(string):
  #print( f"extract_actual_url({string})" )
  arg_string   = string

  # Regex patterns provided by Gemini and its explanation:

  # Query: I need a regular expression to extract a URL from a string of the form
  #        "[https://display_url](https://actual_url)". The regex should return
  #        "https://actual_url"
  # \[.*?\]: Matches the opening square bracket, any characters between the brackets (non-greedy), and the closing square bracket.
  # \(: Matches the opening parenthesis.
  # (.*?): Captures any characters between the parentheses (non-greedy). This is the part that extracts the actual URL.
  # \): Matches the closing parenthesis. 
  regex1 = r"\[.*?\]\((.*?)\)"

  # Query #2: modify the regex to handle the previous case and also this case: "https://display_url](https://actual_url".
  #           The regex should return "https://actual_url"
  # (?:\[.*?\])?: This makes the entire part that matches the square brackets optional.
  # (?:...): This creates a non-capturing group. We don't need to capture the text within the square brackets.
  # ?: This makes the preceding group optional (it can appear zero or one time).
  regex2 = r"(?:\[.*?\])?\((.*?)\)"

  # Find the match in the string
  match = re.search(regex2, string)

  if match:
    # Extract the URL from the captured group
    actual_url = match.group(1)
    return actual_url
  else:
    return arg_string

# Query: none.
# Notes: Written by me because either Gemini couldn't produce a general solution
#        to extract_url() or because I couldn't determine a Query that worked.
def drop_trailing_chars_after_parenthesis(text):
  """Drops all trailing characters after the first closing parenthesis in a string,
     including the closing parenthesis.

  Args:
    text: The input string.

  Returns:
    The string with trailing characters after the first closing parenthesis removed.
  """

  # discard everything after closing round parenthesis
  index = text.find(')')
  if index != -1:
    #return text[:index + 1]   # this will include the closing parenthesis
    return text[:index]       # this will discard the closing parenthesis
  else:
    ## Special cases: URLs in markdown files ending in '<br>'
    index = text.find('>')
    if index != -1:
      return text[:index]       # this will discard the closing parenthesis
    else:
      # special case - last char is a period
      if text[-1] == '.':
        return text[:-1]  # discard last char (known to be a '.' at this point)
      else:
        # special case - last char is a closing square bracket
        if text[-1] == ']':
          return text[:-1]  # discard last char (known to be a ']' at this point)
        else:
          return text

# Query: Generate a python3 function that returns a URL from an input string.
# Notes: This function required multiple queries and Gemini never did provide
#        a solution that worked in all cases.
#
#        Here is an example of a Query that didn't quite work:
#          Please suggest a regex that returns a valid URL from a string.  For
#          example "[formal verification template spreadsheet](https://github.com/openhwgroup/core-v-verif/blob/master/docs/VerifPlans/templates/CORE-V_Formal_VerifPlan_Template.xlsx)."
#          should return "https://github.com/openhwgroup/core-v-verif/blob/master/docs/VerifPlans/templates/CORE-V_Formal_VerifPlan_Template.xlsx".
#          Pay close attention to the trailing characters after the ")".
def extract_url(text):
  """Extracts all URLs from a given text string.

  Args:
    text: The input text string.

  Returns:
    A list of extracted URLs, or an empty list if no URLs are found.
  """

  urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)

  if urls:
    # return urls[0]
    # Me: That is almost correct. The return value should be urls, not urls[0].
    # Gemini: You're absolutely right! Here's the corrected function:
    return urls
  else:
    return None


# Query: Generate a python script that traverses a directory tree and produces
#        a list of files with a specific file extension.
def find_files_by_extension(directory, extension):
  """Finds all files with a specific extension in a directory tree.

  Args:
    directory: The root directory to search.
    extension: The file extension to search for (e.g., '.txt', '.py').

  Returns:
    A list of file paths with the specified extension.
  """

  file_list = []
  for root, _, files in os.walk(directory):
    for file in files:
      if file.endswith(extension):
        file_path = os.path.join(root, file)
        file_list.append(file_path)
  return file_list


# Query: Now write another python function that accepts the list as an input
#        argument and greps for a specific string pattern.
def grep_files(file_list, pattern):
  """Greps for a specific pattern in a list of files.

  Args:
    file_list: A list of file paths.
    pattern: The pattern to search for.

  Returns:
    A list of tuples, where each tuple contains a file path and a list of lines matching the pattern.
  """

  matches = []
  skipped = 0
  for file_path in file_list:
    file_name = os.path.basename(file_path)
    if (args.debug):
      print(file_path)
    if windows_encoded(file_path):
      print(f"Skipping {file_path} because it is Windows encoded")
      skipped += 1
      continue
    if file_name != exclude_file:
      with open(file_path, 'r') as f:
        file_matches = []
        lineno = 0
        for line in f:
          lineno += 1
          if re.search(pattern, line):
            file_matches.append((line.strip(), lineno))
        if file_matches:
          matches.append((file_path, file_matches))
  return matches, skipped

# Query: Generate a python3 function that takes a URL as input argument and
#        tests the URL to see if it exists.
# Notes: Gemini produced a working function based on the Query, but I edited
#        Gemini's code to allow for other responses from the website and to
#        return the actual response code.
def check_url_exists(url):
  """Checks if a URL exists.

  Args:
    url: The URL to check.

  Returns:
    True if the URL exists, False otherwise.
  """

  try:
    response = requests.head(url)
    # status codes 2XX are "success" and 3XX are "redirect"
    # Note that these codes don't always mean what you think they _should_ mean.
    if (response.status_code in range(200, 226) or response.status_code in range(300, 308)):
      return True, response.status_code
    else:
      return False, response.status_code
  except requests.exceptions.RequestException as e:
    #print(f"Error checking URL: {e}")
    return False, 0

###############################################################################
# Script starts here... (this is all human generated code)

file_paths = find_files_by_extension(root_dir, target_extension)

if file_paths:
  matching_files, skipped_files = grep_files(file_paths, search_pattern)
else:
  print("No files with the specified extension found.")
  sys.exit()

valid_urls    = 0
invalid_urls  = 0
first_invalid = 1
if matching_files:
  for file_path, lines_and_line_nums in matching_files:
    if (search_pattern != default_pat):
      print(f"{prefix}{file_path}")
    else:
      for line, num in lines_and_line_nums:
        urls = extract_url(line)
        if (not urls):
          continue
        if (args.debug):
          print(f"URLs : {urls}")
        for url in urls:
          # clean up after the "almost but not quite" regex
          removed_tags = remove_trailing_br_tags(url)
          actual_url = extract_actual_url(removed_tags)
          fixed_url = drop_trailing_chars_after_parenthesis(actual_url)
          # test to see if the URL actually exists
          exists, rcode = check_url_exists(fixed_url)
          if (exists):
            valid_urls += 1
            if (args.debug):
              print (f"    Line {num}, URL {fixed_url} exists (return code = {rcode})")
          else:
            invalid_urls += 1
            if (first_invalid == 1):
              print(f"{prefix}{file_path}")
              first_invalid = 0
            print (f"               Line {num}, URL {fixed_url} does not exist (return code = {rcode})")

      first_invalid = 1

  print ("~~~~~~~~~~ Summary ~~~~~~~~~~")
  print (f"Number of files checked:   {len(file_paths)}")
  print (f"Number of files skipped:   {skipped_files}")
  if (search_pattern != default_pat):
    # only report matches with the pattern
    print (f"Number of matching files: {len(matching_files)}")
  else:
    # report matches, treat as valid URL and report valids
    print (f"Number of files with URLs: {len(matching_files)}")
    print (f"Number of valid URLs:      {valid_urls}")
    print (f"Number of invalid URLs:    {invalid_urls}")
else:
  print("No matches found.")
#end#
